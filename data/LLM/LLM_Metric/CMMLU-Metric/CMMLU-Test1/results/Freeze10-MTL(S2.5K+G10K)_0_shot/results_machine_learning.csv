0,"在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）",F-值:2recallprecision/(recall+precision),G-mean:sqrt(precision*recall),准确性:(TP+TN)/all,AUC:ROC曲线下面积,C,"题目：在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（假设precision=TP/(TP+FP),recall=TP/(TP+FN）。）A. F-值: 2(recallprecision/(recall+precision))B. G-mean: sqrt(precision*recall)C. 准确性: (TP+TN)/allD. AUC: ROC曲线下面积答案是：A. F-值: 2(recallprecision/(recall+precision))。"
1,深度学习中遇见过拟合下列哪个处理办法不可取 ,加dropout层,加深层数,数据增强,加正则项,B,B. 加深层数。深度学习中遇见过拟合，加深层数并不能解决这个问题，过深的网络有梯度消失或梯度爆炸的问题，导致模型泛化能力差。解决过拟合的方法包括：A. 加dropout层：随机丢掉一些神经元，可以减少模型复杂度，提高泛化能力。C. 数据增强：对数据进行变换，如旋转、缩放、平移等，可以增加数据量，减少过拟合。D. 加正则项：在损失函数中增加正则项，可以约束模型复杂度，减少过拟合。
2,假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。1.深度为 4 时将有高偏差和低方差；2.深度为 4 时将有低偏差和低方差。注意：所有其他超参数是相同的，所有其他因子不受影响。,1 和 2,只有 2,没有一个,只有 1,D,D. 只有 1.
3,下列哪些方法不可以用来对高维数据进行降维,LASSO,Bagging,主成分分析法,聚类分析,B,D
4,机器学习中L1正则化和L2正则化的区别是 ,使用L1可以得到稀疏、平滑的权值,使用L2可以得到稀疏、平滑的权值,使用L1可以得到稀疏的权值、使用L2可以得到平滑的权值,使用L2可以得到稀疏的权值、使用L1可以得到平滑的权值,C,"C. 使用L1可以得到稀疏的权值,使用L2可以得到平滑的权值。"
5,下列关于隐马模型和条件随机场模型的说法中错误的是? ,隐马模型和隐马模型隐可用于命名实体识别、分词和词性标注的任务,隐马模型和隐马模型都是生成模型,隐马模型不是概率无向图模型,特征的选取和优化会严重影响隐马模型的结果,B,B
6,下面哪个/些超参数的增加可能会造成随机森林数据过拟合,学习速率,树的数量,树的深度,以上都不是,C,C. 树的深度当树的深度增加时，模型会更好地拟合训练数据，但同时也会增加过拟合的风险。因此，增加树的深度可能会造成随机森林数据过拟合。
7,下面关于迭代二叉树3代算法中说法错误的是,迭代二叉树3代算法是一个二叉树模型,信息增益可以用熵，而不是GINI系数来计算,迭代二叉树3代算法要求特征必须离散化,选取信息增益最大的特征，作为树的根节点,A,A. 迭代二叉树3代算法是一个二叉树模型B. 信息增益可以用熵，而不是GINI系数来计算C. 迭代二叉树3代算法要求特征必须离散化D. 选取信息增益最大的特征，作为树的根节点
8,下面哪个/些选项对 K 折交叉验证的描述是正确的,如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量,更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心,以上都是,增大 K 将导致交叉验证结果时需要更多的时间,C,C. 以上都是K 折交叉验证(K-fold Cross-Validation)是一种评估机器学习模型性能的技巧，通常将数据集分为 K 个部分，其中 K 为一个正整数，然后将 K 次数据子集分别用于验证和测试模型，重复 K 次，最后计算这 K 次测试结果的平均值与验证结果的平均值之比。选项 A 的描述是正确的。如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量。选项 B 的描述也是正确的。较大的 K 值相比于较小的 K 值，将对交叉验证结构有更高的信心，因为较大的 K 值更能够准确地评估模型在未知数据上的表现。选项 D 的描述也是正确的。增大 K 的确将导致交叉验证结果时需要更多的时间。
9,已知有m个样本，进行n（n<=m）次采样。bootstrap数据是什么意思？,无放回地从总共N个样本中抽样n个样本,无放回地从总共M个特征中抽样m个特征,有放回地从总共N个样本中抽样n个样本,有放回地从总共M个特征中抽样m个特征,C,A. 无放回地从总共N个样本中抽样n个样本。Bootstrap sampling 是一种采用自助样本来估计总体参数的方法。它不是从总共M个特征中抽样m个特征。
10,如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有,量纲,已知类别样本质量,以上都不是,分类准则,D,B. 已知类别样本质量。
11,模型训练过程中，我们一般将数据分成 ,验证集,测试集,训练集,其他选项均可,D,C. 训练集
12,你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的,第一个 w1 成了 0，接着 w2 也成了 0,w1 和 w2 同时成了 0,第一个 w2 成了 0，接着 w1 也成了 0,即使在 C 成为大值之后，w1 和 w2 都不能成 0,D,正确答案是 D. 即使在 C 成为大值之后，w1 和 w2 都不能成 0。L1 正则化logistic 回归的公式为：```y = 1 / (1 + e^(-w1*x1 - w2*x2 - C))```其中 y 是预测的 1，x1 和 x2 是输入的 1 和 0，C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当 C 值增加时，正则化项的值会增加，从而使 ReLU 激活函数的值更容易为 0。但是，系数 w1 和 w2 不一定会为 0。当 C 值很大时，只有当 x1 和 x2 都为 0 时，w1 和 w2 才可能为 0。对于其他的 x1 和 x2 的值，w1 和 w2 都不为 0。
13,下列方法中，不可以用于特征降维的方法包括,深度学习SparseAutoEncoder,矩阵奇异值分解SVD,线性判别分析,主成分分析,A,"C. 线性判别分析。线性判别分析(Linear Discriminant Analysis, LDA)是一种分类方法，而不是用于特征降维的方法。LDA主要用于降维后的数据项目分类。其他选项A、B和D都是可以用于特征降维的方法。"
14,贝叶斯定理求得是什么概率? ,先验概率,其它选项都不是,条件概率,联合概率,C,B. 其它选项都不是。贝叶斯定理用于计算后验概率，即在已知一些条件概率下求得其它边缘概率。它并不是用来计算先验概率、条件概率或联合概率的。
15,在机器学习中，解释学习器泛化性能中经常用到偏差-方差分解，下列说法不正确的是 ,方差体现的是学习器预测的稳定性,偏差体现的是学习器预测的准确度,泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的,方差指的是预测的期望值与真实值的偏差,D,选项 D 不正确。方差不是指预测的期望值与真实值的偏差，而是指模型对训练数据的预测结果的波动性。偏差指的是真实值与预测值之间的差异。方差和偏差可以用来衡量模型的预测性能，并且在机器学习中，方差经常用来表示学习器的泛化性能。
16,假如你用logistic Regression 算法去预测电脑销量，当你在新的测试集上验证你的假设时，发现预测值有很大的偏差，并且你的假设在训练集上表现也很差，下面那些步骤你应该避免采纳,尝试着减小正则项 λ,尝试增加交叉特征,增大样本量,尝试更小的测试集或者特征,D,C. 增大样本量
17,以P(w)表示词条w的概率，假设已知P（南京）=0.8，P（市长）=0.6，P（江大桥）=0.4：P（南京市）=0.3，P（长江大桥）=0.5：如果假设前后两个词的出现是独立的，那么分词结果就是,南京_市长_江大桥,南京市_长江_大桥,南京市长_江大桥,南京市_长江大桥,A,C. 南京市长_江大桥
18,对数几率回归（logistics regression）和一般回归分析有什么区别,对数几率回归是设计用来预测事件可能性的,对数几率回归可以用来估计回归系数,以上都是,对数几率回归可以用来度量模型拟合程度,C,C. 以上都是对数几率回归（logistic regression）是一种用于构建二元逻辑模型，用来预测事件可能性的回归分析方法。它可以用来估计回归系数，度量模型拟合程度，以及预测事件可能性。
19,下面哪些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是错误的,类型 1 错误通常在其是正确的情况下拒绝假设而出现,类型 1 通常称之为假正类，类型 2 通常称之为假负类,以上都是,类型 2 通常称之为假正类，类型 1 通常称之为假负类,D,D. 类型 2 通常称之为假正类，类型 1 通常称之为假负类。
20,影响基本K-均值算法的主要因素有,初始类中心的选取,聚类准则,样本输入顺序,模式相似性测度,D,"A. 初始类中心的选取, B. 聚类准则, 和 D. 模式相似性测度。"
21,高斯混合模型(GMM)是一种什么模型,无监督学习模型,其他选项都不是,半监督学习模型,有监督学习模型,A,"D. 有监督学习模型。GMM是一种混合模型, 属于有监督学习模型。它试图通过最大化给定数据集来自动 '假设' 出恰当的 Gaussian 分布族, 并确定每种分布这在数据集中的概率。"
22,基于语法规则的方法为,条件随机场,最大熵模型,句法、语义分析,最大熵隐马尔科夫模型,B,C. 句法、语义分析
23,"有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是",2x-y=0,x+2y=5,x+2y=3,2x+y=4,C,B. x+2y=5
24,隐马尔可夫模型，设其观察值空间为 状态空间为 如果用维特比算法(Viterbi algorithm)进行解码，时间复杂度为,O(NK),O(N^2K),以上都不是,O(NK^2),C,D. O(NK^2)
25,假定你使用了一个很大γ值的RBF核，这意味着：,模型不会被点到超平面的距离所影响,以上都不是,模型仅使用接近超平面的点来建模,模型将考虑使用远离超平面的点建模,C,D. 模型将考虑使用远离超平面的点来建模。γ值越大，RBF核越远离超平面，因此模型将考虑使用远离超平面的点来建模。
26,关于 ARMA (auto regressive moving average model)（自回归滑动平均模型）、 AR (auto regressive model)（自回归模型）、 MA（滑动平均模型） 模型的功率谱，下列说法正确的是,AR模型在零点接近单位圆时，AR谱是一个尖峰,MA模型是同一个全通滤波器产生的,MA模型在极点接近单位圆时，MA谱是一个深谷,RMA谱既有尖峰又有深谷,D,"正确的选项是：A, C。AR模型在零点接近单位圆时，AR谱是一个尖峰，这是AR模型的主要特征，它能够用来预测和分析非平稳信号。MA模型在极点接近单位圆时，MA谱是一个深谷，这是MA模型的主要特征，它能够用来预测和分析非平稳信号。B选项是错误的，因为MA模型和AR模型并不是同一个全通滤波器产生的。D选项也是错误的，因为RMA谱（瑞尔-马赫谱）是一个平滑的谱，它既没有尖峰也没有深谷。"
27,变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做除了下列哪项的变量选择的考虑,交叉验证,变量对于模型的解释有多大作用,特征携带的信息,多个变量其实有相同的用处,B,D. 多个变量其实有相同的用处
28,"下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测",auto regressive model AR模型,自回归滑动平均模型,滑动平均模型,广义自回归滑动平均模型,D,D. 广义自回归滑动平均模型。
29,"当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论",以上都是,"因为Var1和Var2是非常相关的, 我们可以去除其中一个",Var1和Var2是非常相关的,Var3和Var1的1.23相关系数是不可能的,A,C. Var1和Var2是非常相关的。
30,机器学习中做特征选择时，可能用到的方法有,以上都有,卡方,信息增益,期望交叉熵,A,"C. 信息增益特征选择是一种用于机器学习的方法, 目的是在大量特征中选择一个最佳特征集来改善模型的性能。在特征选择时, 可以使用以下方法:- 信息增益: 计算特征与目标变量之间的信息增益, 信息增益越大, 特征的重要性越高。- 卡方: 用于计算类别的方差, 只有在类别数最多的特征才能使用。- 期望交叉熵: 用于计算分类模型的性能, 适用于多类分类问题。因此, 信息增益方法是特征选择的一种可能使用的方法。"
31,LSTM与GRU的一个主要区别在于GRU将LSTM的哪几个gate融合了,forget gate和input gate, input gate和output gate,forget gate和output gate,output gate和reset gate,A,A. forget gate和input gate
32,符号集 a 、 b 、 c 、 d ，它们相互独立，相应概率为 1/2 、 1/4 、 1/8/ 、 1/16 ，其中包含信息量最小的符号是,d,b,a,c,C,A. d
33,假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释,以上都是,log-loss 越低，模型越好,对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大,如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它,A,B. log-loss 越低，模型越好
34,"已知一组数据的协方差矩阵P,下面关于主分量说法错误的是",主分量分析就是K-L变换,"在经主分量分解后,协方差矩阵成为对角矩阵","主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小",主分量是通过求协方差矩阵的特征值得到,A,C. 主分量分析的最佳准则是对一组数据进行按一组正交基分解， 在只取相同数量分量的条件下，以均方误差计算截尾误差最小。
35,下列关于attention机制的说法错误的是,attention机制会给序列中的元素分配一个权重系数,attention机制可以用于机器阅读理解、问答对话等场景中,传统encoder-decoder模型存在长距离依赖问题,attention机制的变体，多头attention机制不适合并行，其每一步计算依赖于上一步的计算结果,D,D
36,以下哪项不是降低过拟合的方法,收集更多训练数据,进行数据清洗，减少噪声,增加神经网络隐藏层节点数,简化模型假设,C,C. 增加神经网络隐藏层节点数增加神经网络隐藏层节点数会增加模型的复杂度，导致模型在训练数据上表现得更好，但可能导致在测试数据上表现不佳，从而加剧过拟合。其他选项（收集更多训练数据、进行数据清洗减少噪声和简化模型假设）都可以减少过拟合。
37,"给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么","D1 = C1, D2 < C2, D3 < C3","D1= C1, D2 < C2, D3 > C3","D1 = C1, D2 = C2, D3 = C3","D1 = C1, D2 > C2, D3 > C3",C,"B. D1 = C1, D2 < C2, D3 < C3"
38,在决策树中，用作分裂节点的information gain说法不正确的是,信息增益更加倾向于选择有较多取值的属性,信息增益可以使用熵得到,较小不纯度的节点需要更多的信息来区分总体,以上均不是,C,D. 以上均不是。在决策树中，信息增益（也称为增益）是指一个节点将其属性分为两个子集后，所产生的信息量减去原始信息量的比例。信息增益通常用来选择最优的分裂点。选项A、B和C都是关于信息增益的描述，因此它们都是正确的。
39,下列关于回归分析中的残差表述正确的是,残差的平均值总小于零,残差的平均值总大于零,残差的平均值总为零,残差没有此类规律,C,C. 残差的平均值总为零。
40,"我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 ","我们随机抽取一些样本, 在这些少量样本之上训练",以上所有,我们可以试用在线机器学习算法,"我们应用PCA算法降维, 减少特征数",B,"D. 我们应用PCA算法降维, 减少特征数"
41,下列哪个不属于条件随机场模型对于隐马尔科夫模型和最大熵隐马尔科夫模型模型的优势,速度快,可容纳较多上下文信息,全局最优,特征灵活,A,C. 全局最优。条件随机场模型、最大熵条件随机场模型和最大熵隐马尔可夫模型是三种不同的模型，它们各自具有不同的优势。- 条件随机场模型：优点是计算简便，可容纳较多上下文信息，特征灵活；缺点是速度快。- 最大熵条件随机场模型是条件随机场模型的扩展，它增加了寻找最大熵的约束，具有更好的鲁棒性和更强的拟合能力，能够更好的表示隐含在文本中的因果关系。- 最大熵隐马尔可夫模型是隐马尔可夫模型的一种，它假设观察序列是马尔可夫过程的生成序列，最大熵隐马尔可夫模型在此基础上添加了熵的约束，用以获取更优的隐含状态序列。综上所述，最大熵隐马尔可夫模型、条件随机场模型和最大熵条件随机场模型都具有各自的优点，没有一种模型是全局最优的，所以选项C是错误的。
42,下列哪项不是基于词典的方法的中文分词的基本方法,最大熵模型,最大概率法,最大匹配法,最短路径法,A,D
43,假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。 如果使用较小的C（C趋于0），则：,不确定,误分类,正确分类,以上均不正确,B,A. 不确定。解析：当C趋于0时，二次核函数退化为线性核函数，此时SVM不再能够进行分类，而是产生一个边界。对于误分类的情况，需要进一步分析边界上的数据点，可以采用一些分类方法进行处理。
44,以下哪种方法属于生成模型,条件随机场,传统神经网络,朴素贝叶斯,线性回归,C,A. 条件随机场
45,在其它条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题,增加训练集数量,删除稀疏的特征,SVM算法中使用高斯核/RBF核代替,减少神经网络隐藏层节点数,C,选项D. 减少神经网络隐藏层节点数。过拟合问题是指模型在训练数据上表现得非常好，但在未知数据上表现较差。减少神经网络隐藏层节点数可以减少模型的复杂性，从而降低过拟合的风险。其他选项中，增加训练集数量、删除稀疏的特征和SVM算法中使用高斯核/RBF核代替都可以减少过拟合问题。
46,对应GradientBoosting tree算法， 以下说法正确的是,当增加最小样本分裂个数，我们可以抵制过拟合,当我们减少训练单个学习器的样本个数，我们可以降低偏差,当增加最小样本分裂个数，会导致过拟合,当我们增加训练单个学习器的样本个数，我们可以降低方差,A,"B. 当我们减少训练单个学习器的样本个数, 我们可以降低偏差。"
47,假设你训练SVM后，得到一个线性决策边界，你认为该模型欠拟合。在下次迭代训练模型时，应该考虑,减少训练数据,减少特征,计算更多变量,增加训练数据,C,D. 增加训练数据
48,语音信号由于具有什么特性，所以我们可以将语音信号进行分窗处理? ,随机单调性,其他选项都不是,短时平稳性,单调不变性,C,C. 短时平稳性。语音信号具有短时平稳性，即短时内具有较小变化，所以我们可以将语音信号进行分窗处理，在窗口内作近似处理。
49,"对于k折交叉验证, 以下对k的说法正确的是","选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)","k越大, 不一定越好, 选择大的k会加大评估时间","在选择k时, 要最小化数据集之间的方差",以上所有,D,"D. 以上所有k折交叉验证是一种评估模型好坏的不确定性方法, 选择较大的k值可以减少测试数据与训练数据之间的方差, 从而减小偏差。但k值并不是越大越好，当数据量较小的时候，选择较小的k值可以减少评估时间。所以在选择k值时，需要综合考虑偏差和评估时间。选项D是正确的。"
50,以下属于欧式距离特性的有,尺度缩放不变性,旋转不变性,不受量纲影响的特性,考虑了模式的分布,B,D. 考虑了模式的分布 (选项B和C是错误的)
51,"朴素贝叶斯是一种特殊的贝叶斯分类器,特征变量是X,类别标签是C,它的一个假定是",特征变量X的各个维度是类别条件独立随机变量,P(X|C)是高斯分布,以0为均值，sqr(2)/2为标准差的正态分布,各类别的先验概率P(C)是相等的,A,A. 特征变量X的各个维度是类别条件独立随机变量
52,在一个n维的空间中， 最好的检测outlier(离群点)的方法是,作盒形图,作散点图,作正态分布概率图,马氏距离,D,D. 马氏距离。
53,对于线性回归模型，包括附加变量在内，以下的可能正确的是 ,R-Squared 是递减的， Adjusted R-squared 也是递减的,R-Squared 是常量的，Adjusted R-squared是递增的,R-Squared 和 Adjusted R-squared都是递增的,以上都不是,D,C. R-Squared 和 Adjusted R-Squared 都是递增的。 对于线性回归模型的评估，R-Squared 描述了模型解释的数据变异的比例。R-Squared 以及 Adjusted R-Squared 调整过模型误差，去除了模型中自变量的影响，可以更准确地评估模型的预测能力。通常情况下，R-Squared 和 Adjusted R-Squared 都是递增的，说明模型解释的变异部分随着自变量的增加而增加。
54,数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是,单个模型之间有高相关性,单个模型都是用的一个算法,单个模型之间有低相关性,在集成学习中使用“平均权重”而不是“投票”会比较好,C,C. 单个模型之间有低相关性。
55,"我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以",增加学习率 ,减少树的数量,增加树的深度,减少树的深度,D,D. 减少树的深度
56,"我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案",以上所有,"我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征",使用前向特征选择方法和后向特征排除方法,"查看相关性表, 去除相关性最高的一些特征",A,C. 使用前向特征选择方法和后向特征排除方法。
57,以下哪个激活函数不能解决梯度弥散的问题,Leaky-Relu,Elu,Sigmoid,Relu,C,C. Sigmoid。Sigmoid函数为: σ(x) = 1 / (1 + exp(-x))，它只有一个分支，无法解决梯度弥散的问题。
58,下列哪个不属于常用的文本分类的特征选择算法,主成分分析,互信息,信息增益,卡方检验值,A,A. 主成分分析。
59,Fisher线性判别函数的求解过程是将M维特征矢量投影在（ ）中进行求解,一维空间,三维空间,M-1维空间,二维空间,A,A. 一维空间。
60,下列哪一项说明了X，Y之间的较强关系,相关系数为0.9,都不对,Beta系数为0的空假设的p-value是0.0001,Beta系数为0的空假设的t统计量是30,A,由于你提供的选项中没有X和Y的描述和解释，所以我无法给出正确答案。请提供更多关于X和Y的信息，以便我能给出正确答案。
61,在 k-均值算法中，以下哪个选项可用于获得全局最小？,以上所有,找到集群的最佳数量,调整迭代的次数,尝试为不同的质心（centroid）初始化运行算法,A,"没有正确答案的选项。 在 k-均值算法中, 没有明确的选项用于获得全局最小。 它是一种迭代算法, 用于找到k个质心, 并不断更新数据点到最靠近的质心。 该算法通过最小化每个数据点到其质心的距离来不断改进全局最小。 没有单一的“最佳”质心初始化方法或参数调整方法, 因为它们的适用性会根据数据和问题而异。"
62,在统计语言模型中，通常以概率的形式描述任意语句的可能性，利用最大相似度估计进行度量，对于一些低频词，无论如何扩大训练数据，出现的频度仍然很低，下列哪种方法能解决这一问题,数据平滑,N元文法,一元文法,一元切分,A,A. 数据平滑。对于低频词，无论如何扩大训练数据，出现的频度仍然很低。数据平滑的方法是在训练数据集中添加一些概率很小的常数，以增加训练数据中词的频率。这样，在训练过程中，低频词的概率就会相对提高。
63,以下说法中错误的是,给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的,boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重,SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性,在adaboost算法中，所有被分错样本的权重更新比例不相同,B,A. 给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的B. boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重C. SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性D. 在adaboost算法中，所有被分错样本的权重更新比例不相同
64,"一个二进制源X发出符号集为{-1,1}，经过离散无记忆信道传输，由于信道中噪音的存在，接收端Y收到符号集为{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求条件熵H(Y|X)",0.5372,0.2375,0.5273,0.3275,B,B. 0.2375
65,以下哪种技术对于减少数据集的维度会更好？,删除数据差异较大的列,删除缺少值太多的列,删除不同数据趋势的列,都不是,B,D. 都不是以上选项都不是特别适合用于减少数据集维度。 在某些情况下，删除数据差异较大的列可能会有所帮助，但这取决于这些差异是否与目标相关。 删除缺少值太多的列可能会使数据集过拟合，因为它可能减少变异性。 删除不同数据趋势的列也可能会消除数据的相关性，从而降低模型的预测能力。在reduce dimensionality(降维)时，使用pca（主成分分析），lDA（线性 discriminant分析）等方法来选择特征，通常会比较有用。
66,下列哪些不特别适合用来对高维数据进行降维,聚类分析,LASSO,小波分析法,拉普拉斯特征映射,A,A
67,逻辑回归与多元回归分析有哪些不同？,逻辑回归回归系数的评估,逻辑回归预测某事件发生的概率,逻辑回归有较高的拟合效果,以上全选,D,D. 以上全选逻辑回归与多元回归分析的不同之处在于：- 逻辑回归是分类问题，多元回归是连续型变量预测。- 逻辑回归中，事件可以是成功也可以是失败，而多元回归中只讨论连续变量。- 逻辑回归中需要构建逻辑回归方程，多元回归不需要。- 逻辑回归中，需要对模型进行优化，而多元回归只需要求解系数。- 逻辑回归通常使用ROC曲线来评估模型，而多元回归通常使用回归系数。- 逻辑回归的系数表示事件发生的概率，而多元回归的系数表示自变量对因变量的影响程度。
68,最出名的降维算法是 PCA 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的,两个都在最近邻空间能得到解释,X_projected_PCA 在最近邻空间能得到解释,两个都不能在最近邻空间得到解释,X_projected_tSNE 在最近邻空间能得到解释,D,由于 PCA 和 t-SNE 都是无监督的降维算法，所以不能在最近邻空间得到解释。因此选项 C 是正确的。
69,下列关于维特比算法(Viterbi)的说法中错误的是,维特比算法中的转移概率是从一个隐含状态转移到另一个隐含状态的概率,维特比算法是一种贪心算法,维特比算法可应用于中文分词任务,维特比算法可得到全局最优解,B,D
70,以下( )不属于线性分类器最佳准则,贝叶斯分类,感知准则函数,支持向量机,Fisher准则,A,D. Fisher准则。
71,对于线性回归，我们应该有以下哪些假设,"找到离群点很重要, 因为线性回归对离群点很敏感",线性回归假设数据没有多重线性相关性,线性回归要求所有变量必须符合正态分布,以上都不是,D,D. 以上都不是。
72,下面不是迭代二叉树3代算法对数据的要求,所有的训练例的所有属性必须有一个明确的值,所有属性必须为离散量,所有属性必须为连续,相同的因素必须得到相同的结论且训练例必须唯一,C,D. 相同的因素必须得到相同的结论且训练例必须唯一。迭代二叉树3代算法要求相同的因素必须得到相同的结论，即对于树中的相同节点，其对应的训练例必须在算法处理的过程中得到相同的处理结果。此外，这些训练例必须在算法处理的过程中唯一，即不同的训练例不应该被混淆。 A选项中，所有的训练例的所有属性必须有一个明确的值，这个描述并不准确，因为训练例的属性可以是连续的，也可以是离散的。 B选项中，所有属性必须为离散量，也不准确，因为训练例的属性既可以是离散的，也可以是连续的。 C选项中，所有属性必须为连续，同样不准确，因为训练例的属性既可以是连续的，也可以是离散的。
73,下面的优化算法中，速度最快的是 ,BFGS,梯度下降法,牛顿法,Adam,C,D. Adam。
74,下列关于ALBERT的说法不正确的是 ,跨层参数共享,采用词嵌入向量参数的因式分解,应用在下游任务中预测速度显著提速,去掉了dropout,D,B. 采用词嵌入向量参数的因式分解
75,下面哪个属于SVM应用,新文章聚类,文本和超文本分类,图像分类,以上均是,D,D. 以上均是。
76,类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是,基于二次准则的H-K算法,感知器算法 ,势函数法,伪逆法,B,C. 势函数法在类域界面方程法中，基于二次准则的H-K算法、感知器算法和伪逆法都可以用于求解线性不可分的分类问题的近似或精确解。但是，势函数法不适用于此问题。
77,下面哪个选项中哪一项属于确定性算法,K-Means,PCA,KNN,以上都不是,B,D. 以上都不是A. K-MeansB. PCAC. KNN以上三个选项都不属于确定性算法。在机器学习领域，确定性算法（Deterministic Algorithm）指的是输出结果和输入数据大小无关的算法，通常经过多次实验结果的平均或投票来决定最终结果。然而，在机器学习领域中，很难找到确定性算法，大多数算法的结果都依赖于输入数据和模型参数。
78,"以下哪些算法, 1. KNN；2. 线性回归；3.对数几率回归。可以用神经网络去构造:",2 和 3,1和 2,以上都不是,"1, 2 和 3",A,以上都是。神经网络是一种模拟人脑神经元结构和功能的计算模型，可以用于构造许多不同类型的算法，包括KNN、线性回归和对数几率回归等。
79,训练SVM的最小时间复杂度为O(n2)，那么一下哪种数据集不适合用SVM?,和数据集大小无关,大数据集,小数据集,中等大小数据集,B,最小时间复杂度为O(n²)的数据集，即二次规划问题，对于分类问题来说，就是决策边界的超平面与数据集的样本点越接近，则需要分类的最优解越准确，边界的最优划分效果越好。对于二次规划问题，数据集中的点越密集，则决策边界越平滑，与真实分类界限的偏差越小，与真实分类界限边界越近，但并不是说适合用SVM。因此，选项A，和数据集大小无关是不正确的。正确答案是选项B，大数据集。
80,如果线性回归模型中的随机误差存在异方差性，那么参数的普通最小二乘法估计量是,无偏的，非有效的,无偏的，有效的,有偏的，非有效的,有偏的，有效的,A,C. 有偏的，非有效的
81,下列关于RoBERTa的说法不正确的是 ,不做NSP任务,采用静态掩码机制,采用更多训练数据,训练采用更大batch size,B,B. 采用静态掩码机制。
82,在逻辑回归输出与目标对比的情况下，以下评估指标中哪一项不适用？,准确度,均方误差,AUC-ROC,Logloss,B,B. 均方误差
83,语言模型的参数估计经常使用MLE（最大似然估计）。面临的一个问题是没有出现的项概率为0，这样会导致语言模型的效果不好。为了解决这个问题，需要使用（）,增加白噪音,平滑,随机插值,去噪,B,B. 平滑。平滑方法可以处理语言模型中没有出现的项概率为0的问题，从而提高模型的效果。
84,建模北京市人口的年龄分布，采用什么分布更合适,0-1分布,正态分布,泊松分布,指数分布,B,"C. 泊松分布。 北京市人口的年龄分布通常不符合泊松分布、指数分布和0-1分布, 因此 泊松分布是一个常用的假设检验分布，用于 modeling北京市人口的年龄分布更为合适。"
85,SVM中的代价参数表示：,误分类与模型复杂性之间的平衡,以上均不是,使用的核,交叉验证的次数,A,A. 误分类与模型复杂性之间的平衡。
86,关于SVM泛化误差描述正确的是,超平面与支持向量之间距离,SVM的误差阈值,以上都不是,SVM对未知数据的预测能力,D,C. 以上都不是。SVM的泛化误差是指在训练数据集上表现得很好的SVM模型，在未知数据上表现得并不好的情况。它是由训练数据中的噪声或异常值引起的。超平面与支持向量之间距离不是泛化误差的描述，而是一种用于选择最优超平面的度量。SVM的误差阈值和SVM对未知数据的预测能力也不是泛化误差的描述。
87,下列关于BERT的说法不正确的是 ,支持对语义上下文进行建模,采用激活函数GELU,网络一共有20层,使用transformer,C,C. 网络一共有20层。BERT网络通常有768层，不是20层。
88,模式识别中，不属于马式距离较之于欧式距离的优点的是,尺度不变性,平移不变性,考虑到各种特性之间的联系,考虑了模式的分布,B,C. 考虑到各种特性之间的联系。
89,描述的机器发生故障的次数，采用什么分布更合适? ,0-1分布,指数分布,正态分布,泊松分布,D,D. 泊松分布泊松分布适合描述机器发生故障的次数，因为它是一个离散分布，适用于独立事件发生的次数。
90,以下哪个不是LSTM本身的特点 ,LSTM是RNN的一种变种,防止梯度弥散,训练时GPU使用率较高,LSTM有遗忘门,C,C. 训练时GPU使用率较高
91,关于逻辑回归和支持向量机不正确的是,逻辑回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。逻辑仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率,支持向量机可以通过正则化系数控制模型的复杂度，避免过拟合。,支持向量机的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化,逻辑回归的输出就是样本属于正类别的几率，可以计算出概率。,A,C
92,以下不属于影响聚类算法结果的主要因素有,特征选取,已知类别的样本质量,分类准则,模式相似性测度,B,A. 特征选取
93,高斯混合模型(GMM)采用什么准则进行训练,均方误差最小化,经验风险最小化,期望最大化,其他选项都不是,C,C. 期望最大化
94,以下说法正确的是,一个机器学习模型，如果有较高准确率，并不难总是说明这个分类器是好的,我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习,如果增加模型复杂度， 那么模型的测试错误率总是会降低,如果增加模型复杂度， 那么模型的训练错误率总是会降低,A,由于没有给出选项，我无法给出正确答案的选项。 如果您提供选项，我就可以回答您的问题。
95,中文同义词替换时，常用到Word2Vec，以下说法错误的是,Word2Vec结果符合当前预料环境,Word2Vec得到的都是语义上的同义词,Word2Vec受限于训练语料的数量和质量,Word2Vec基于概率统计,B,A. Word2Vec结果符合当前预料环境解释：Word2Vec是一种用于生成词向量的机器学习方法，它并不保证结果符合当前预料环境。
96,"在以下不同的场景中,使用的分析方法不正确的有","根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级","根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式","用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫","根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女",B,题目没有提供选项，所以我无法给出正确答案的选项。但是我可以告诉你，选项A、C和D都可能是正确的。选项A根据商家最近一年的经营及服务数据，用聚类算法判断出天猫商家在各自主营类目下所属的商家层级是合适的。选项C用关联规则算法分析出购买了汽车坐垫的买家是否适合推荐汽车脚垫也是合适的。选项D根据用户最近购买的商品信息，用决策树算法识别出淘宝买家可能是男还是女也是一个合适的方法。因此，正确答案取决于具体的分析方法和场景。
97,在数据清理中，下面哪个不是处理缺失值的方法,变量删除,估算,整例删除,成对删除,D,C. 整例删除。
98,以下关于LDA(Latent Dirichlet allocation)的说法错误的是,LDA是非监督学习技术,LDA可通过EM的思想求解,当选取一篇文档后，对于该文档主题的分布是确定的,LDA包含词，主题和文档三层结构,C,C
99,如果我使用数据集的全部特征并且能够达到100%的准确率，但在测试集上仅能达到70%左右，这说明：,以上均不正确,欠拟合,过拟合,模型很棒,C,C. 过拟合
100,假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。 当你使用较大的C（C趋于无穷），则：,以上均不正确,不确定,不能正确分类,仍然能正确分类数据,D,D. 仍然能正确分类数据。当使用较大的C（C趋于无穷）时，SVM将趋近于线性分类器，此时二次核函数的近似线性形式近似于线性可分的学习器，此时SVM可以完成分类。
101,基于统计的分词方法为,正向量最大匹配法,条件随机场,最少切分,逆向量最大匹配法,B,C. 最少切分
102,假定某同学使用假定某同学使用朴素贝叶斯分类模型时，不小心将训练数据的两个维度搞重复了，那么关于朴素贝叶斯的说法中不正确的是,模型效果相比无重复特征的情况下精确度会降低,模型效果相比无重复特征的情况下精确度会提高,当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题,如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样,D,A. 模型效果相比无重复特征的情况下精确度会降低。
103,下列关于word2vec的说法中错误的是,使用词向量可得到以下等式:King - man + woman = Queen,Skip-gram是给定词窗中的文本，预测当前词的概率,word2vec的假设是词袋模型，词的顺序是不重要的,word2vec训练中使用了Negative Sample与Hierarchical Softmax两种加速算法,B,C. word2vec的假设是词袋模型，词的顺序是不重要的。这个选项是错误的。word2vec的假设是词袋模型，但是词的顺序是很重要的。训练过程中，word2vec会根据词的上下文来学习词向量，词的顺序对于学习到的词向量有很大影响。
104,以下描述正确的是 ,聚类分析可以看作是一种非监督的分类。,在聚类分析当中，簇内的相似性越大，簇间的差别越大，聚类的效果就越差。,SVM是这样一个分类器，他寻找具有最小边缘的超平面，因此它也经常被称为最小边缘分类器,在决策树中，随着树中结点数变得太大，即使模型的训练误差还在继续减低，但是检验误差开始增大，这是出现了模型拟合不足的问题。,A,A. 聚类分析可以看作是一种非监督的分类。
105,"下列选项中,识别模式与其他不⼀样的是",出⾏方式判断:步⾏、骑车、坐车,⽤户年龄分布判断:少年、青年、中年、⽼年,投递员分拣信件,医⽣给病⼈诊断发病类型,A,A. 出行方式判断:步行、骑车、坐车
106,下列说法不正确的是,梯度下降法是利用当前位置的负梯度作为搜索方向的方法,共轭梯度法仅需利用一阶导数的信息，但是收敛速度高于梯度下降法,批量梯度下降和随机梯度下降相比，批量梯度下降优势是对于大规模样本效率很高,牛顿法和梯度下降法相比，一个劣势是求解复杂，一个优势是收敛速度加快,C,"A. 梯度下降法是利用当前位置的负梯度作为搜索方向的方法B. 共轭梯度法仅需利用一阶导数的信息,但是收敛速度高于梯度下降法C. 批量梯度下降和随机梯度下降相比,批量梯度下降优势是对于大规模样本效率很高D. 牛顿法和梯度下降法相比,一个劣势是求解复杂,一个优势是收敛速度加快答案是B."
107,"在隐马尔科夫模型中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计",前向后向算法,极大似然估计 ,维特比算法,EM算法,B,D. EM算法。
108,LDA(Latent Dirichlet allocation)中归属于同一主题下单词分布的先验分布是? ,正态分布,狄利克雷分布,多项分布,二项分布,C,B. 狄利克雷分布。
109,线性回归的基本假设不包括哪个,对于解释变量的所有观测值，随机误差项有相同的方差,随机误差项是一个期望值为0的随机变量,随机误差项服从正态分布,随机误差项彼此相关,D,D. 随机误差项彼此相关。
110,下列不是SVM核函数的是,Sigmoid核函数,径向基核函数,多项式核函数,logistic核函数,D,D. logistic核函数
111,下列哪些是非监督学习方法 ,SVM,K-means,KNN,决策树,B,D
112,以下哪种方法属于判别式模型,贝叶斯网络,朴素贝叶斯,隐马模型 ,支持向量机,D,B. 朴素贝叶斯。
113,下面的交叉验证方法：i. 有放回的Bootstrap方法；ii. 留一个测试样本的交叉验证；iii. 5折交叉验证；iv. 重复两次的5折教程验证。当样本是1000时，下面执行时间的顺序，正确的是,ii > iv > iii > i,ii > iii > iv > i,iv > i > ii > iii,i > ii > iii > iv,A,C. iv > i > ii > iii
114,Seq2Seq模型在解码时可以选用的方法 ,贪心算法,二者均可,Beam Search,二者均不可,B,C. Beam SearchSeq2Seq模型在解码时可以选用Beam Search方法。Beam Search是一种概率搜索方法，用于在解码过程中快速采样解码概率，并基于这些概率对未来tokens进行预测，从而在解码过程中实现更好的长短时记忆。
115,解决隐马模型中预测问题的算法是,前向算法,维特比算法,Baum-Welch算法,后向算法,B,B. 维特比算法
116,以下对k-means聚类算法解释正确的是,"能自动识别类的个数,不是随即挑选初始点为中心点计算","不能自动识别类的个数,不是随即挑选初始点为中心点计算","不能自动识别类的个数,随即挑选初始点为中心点计算","能自动识别类的个数,随即挑选初始点为中心点计算",C,C. 不能自动识别类的个数，随即挑选初始点为中心点计算。
117,一般，k-NN最近邻方法在（）的情况下效果较好,样本呈团状分布,样本较多但典型性不好,样本呈链状分布,样本较少但典型性好,D,D. 样本较少但典型性好。
118,一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求,多分类问题,二分类问题,k-中心点聚类问题,层次聚类问题,A,A. 多分类问题。
119,为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做,将数据转换成零均值,无法做到,将数据转换成零众数,将数据转换成零中位数,A,"C. 将数据转换成零众数。PCA (主成分分析) 和 SVD (奇异值分解) 都可以用于降维, 即将高维数据转换为低维数据, 减少数据的维度，从而减少计算量。PCA 中的项目向量化是利用线性变换将随机向量投影到新的坐标系中使得新的坐标系下各个项目的方差最大化。而 SVD 是通过分解矩阵来求解最陡峭的线性变换。对于薄数据（身份特征值少，样本数少），SVD 可以得到最好的结果；对于厚数据（身份特征值多，样本数多），PCA 可以得到最好的结果。"
120,在统计模式分类问题中，当先验概率未知时，可以使用,N-P判决,最小最大损失准则,最小损失准则,最小误判概率准则,B,D. 最小误判概率准则。
121,以下哪些方法不可以直接来对文本分类,决策树,Kmeans,支持向量机,KNN,B,B. Kmeans。选项B的Kmeans算法是一种聚类算法，可以对数据进行分类，但它不适用于文本分类。文本分类通常使用NLP（自然语言处理）技术，利用词袋模型、TF-IDF、词嵌入等技术对文本进行特征提取，然后使用朴素贝叶斯、支持向量机、逻辑回归等分类算法对文本进行分类。
